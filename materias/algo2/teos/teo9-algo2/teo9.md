---
layout: page
title: Te√≥rica 9
permalink: /materias/algo2/teo9/
---

# Sorting - Ordenamiento

El ordenamiento consiste en, dado un arreglo de tipo $\alpha$, para el que est√° definida la relaci√≥n $<_\alpha$, queremos ordenarlo de menor a mayor o mayor a menor.

Es uno de los problemas m√°s cl√°sicos, √∫tiles y estudiados.

## 1. Selection Sort

Es el que vamos buscando el m√≠nimo y lo ponemos al principio. Luego buscamos el m√≠nimo para el resto del arreglo y lo ubicamos en la segunda posici√≥n. Y as√≠ sucesivamente hasta el √∫ltimo elemento.

**Algoritmo**

Para todo i desde 0 a n

seleccionar el m√≠nimo entre la posici√≥n actual (i) y el final

ubicarlo en la posici√≥n i

El algoritmo termina y lo hace devolviendo un arreglo ordenado.

### Invariante del algoritmo

En cualquier paso:

- Desde 0 hasta i ya est√° ordenado el arreglo.
- Todas las posiciones que quedan despu√©s de i tienen que ser mayores a las que quedan antes. Y viceversa.
- El arreglo es una permutaci√≥n del arreglo original.

Tiene complejidad $O(n^2)\;$$=\frac{n(n-1)}{2}$.

El costo no depende de nada. Siempre es igual. O sea, no importa si el algoritmo ya est√° ordenado o no. Cuesta lo mismo en el peor y en el mejor caso.

## 2. Insertion Sort

Supongamos que queremos ordenar de menor a mayor. 

Agarramos el segundo elemento y lo movemos hacia atr√°s hasta que el que est√© a derecha sea mayor. Luego hacemos lo mismo para el elemento 3 y as√≠ sucesivamente hasta n.

### Invariante

- Los elementos de 0 a i son los elementos de 0 a i originales del arreglo.
- Los elementos de 0 a i est√°n ordenados.
- El arreglo es una permutaci√≥n del arreglo original

```cpp
**para** i desde 1 hasta n-1 **hacer**
	instertar(i)

insertar(i):
**para** i desde 1 hasta n-1 **hacer**
j = i-1
elem = a[i]

**mientras** j‚â•0 and a[j]>elem **hacer**
	a[j+1] = a[j]
	j = j-1

a[j+1] = elem
```

Costo $\frac{(n-1)(n-2)}{2}$ $= O(n^2)$.

Si el arreglo est√° parcialmente ordenado, el costo mejora. Y si est√° ordenado al rev√©s es el peor caso.

Se dice que un algoritmo es **estable** si mantiene el orden anterior de elementos con igual clave. 

En un algoritmo **estable,** dos claves iguales tienen que mantener el orden que ten√≠an al principio

![Teo%209%20Algo%202%20844575a5671b427db8b7d07ae97e8502/Untitled.png](Teo%209%20Algo%202%20844575a5671b427db8b7d07ae97e8502/Untitled.png)

Si un algoritmo es inestable, puede mantener el orden pero tambi√©n puede no hacerlo. 

¬øPara qu√© sirve? Cuando trabajamos con claves con m√∫ltiples valores, puede ser que en alg√∫n caso queramos que las segundas claves de los elementos que coinciden preserven el orden al ordenar los elementos seg√∫n la primera clave.

---

**Ejemplo:**

Nos dan este arreglo ordenado alfab√©ticamente y queremos ordenarlo seg√∫n el turno. Pero queremos un algoritmo estable as√≠ no cambiamos el orden alfab√©tico inicial.

![Teo%209%20Algo%202%20844575a5671b427db8b7d07ae97e8502/Untitled%201.png](Teo%209%20Algo%202%20844575a5671b427db8b7d07ae97e8502/Untitled%201.png)

---

¬øSelection e Insertion Sort son estables?

Selection Sort ‚Üí **No**, porque el m√≠nimo lo swapeamos con el inicial as√≠ nom√°s.

Insertion Sort ‚Üí **S√≠**, depende de cuando frenemos en cada paso pero s√≠.

---

## 3. HeapSort

Hasta ahora vimos el Selection y el Insertion. Ambos no muy eficientes.

Siguiendo la l√≠nea de Selection Sort, queremos una estructura que nos ayude a encontrar el m√≠nimo m√°s r√°pidamente (en vez de recorrer toda la lista).

Ah√≠ aparece el Heap. Podemos ir metiendo los elementos en un heap y luego ir sac√°ndolos.

Pero el **HeapSort** es todav√≠a mejor. Usa la operaci√≥n Array2Heap, que tiene complejidad $O(n^2)$.

Pasamos de $O(n^2)$ a $O(n.log(n))$. 

```cpp
heapSort(A):
	heap = Array2Heao(A)
	for i desde n-1 hasta 0 hacer
		max = proximo(heap)
		desencolar(heap)
		A[i] = max
	return A

Costo: O(n) + O(n*log(n))
```

En este caso usamos un m√°x-Heap para ordenar un arreglo de menor a mayor.

¬øEs estable? ¬øSe puede hacer estable?

---

## 4. Merge Sort

El m√°s "elegante" üëÄ üëÄ.

Usa la metodolog√≠a $Divide\;\&\;Conquer.$ 

Consiste en dividir un problema en problemas similares pero m√°s chicos, leugo resolver esos problemas menores y combinar las soluciones de los problemas menores para obtener la soluci√≥n del problema original.

Tiene sentido siempre y cuando la divisi√≥n y la combinaci√≥n no sean caras (y el problema se reduzca en cada paso).

```cpp
mergeSort(A){
	if (n<2){
		el arreglo ya est√° ordenado
	}

	else{
		dividir el arreglo en 2 partes iguales
		ordenar_recursivamente ambas mitades //mergeSort(A1) y A2.
		combinar ambas mitades en un √∫nico arreglo //combinar(A1, A2).
	}

}
```

El costo del algoritmo es de $O(n.log(n))$.

---

## 5. Quick Sort

Idea m√°s o menos parecida al MergeSort (con D&Q).

En mergeSort la divisi√≥n se hac√≠a en la mitad del arreglo y se ordenaban ambas mitades recursivamente.

En vez de cortar el elemento por la mitad, lo cortamos por el elemento mediano. (O sea el que tiene el valor mediano).

```cpp
quickSort(A){
	separamos el arreglo en 2 partes. Una mitar con los
	los elementos menores al mediano y la otra con los mayores

	ordenamos las 2 partes

}
```

¬øQu√© pasa si no conocemos el elemento mediano? Mmm.

Una opci√≥n es usar un algoritmo para encontrar el mediano, pero le va a sumar mucho costo a nuestro algoritmo (aunque sea de costo lineal).

Lamentablemente, no podemos usar como cota superior del peor caso
 $O(n*log(n))$, porque podemos elegir mal el pivote y que quede $O(n^2)$.

Peor caso: $O(n^2)$.

Caso promedio: $O(n*log(n))$.

Una forma de zafar de los casos peores es elegir por ejemplo 5 elementos y elegir el mediano de esos como pivote. Por lo menos no vamos a elegir ninguno de los extremos. Funciona bien en la pr√°ctica.

---

# Complejidades

Merge Sort y Heap Sort: $O(n*log(n))$.

Quick Sort, Selection Sort, Insertion Sort: $O(n^2)$.

<aside>
‚ö†Ô∏è Quick Sort en mejor caso: $O(n*log(n))$.
Insertion Sort en mejor caso: $O(n).$

</aside>

**¬øCu√°l es la eficiencia m√°xima (complejidad m√≠nima) obtenible en el caso peor?**

$\Omega(n*log(n))$ es lo m√≠nimo que vamos a tardar en el caso peor. No van a existir otros algoritmos que sean m√°s r√°pidos. Los algoritmos con esa complejidad van a ser √≥ptimos.

Las cotas inferiores son m√°s dif√≠ciles de poner que las superiores.

Justificaciones te√≥ricas usando √°rboles de decisi√≥n.

![Teo%209%20Algo%202%20844575a5671b427db8b7d07ae97e8502/Untitled%202.png](Teo%209%20Algo%202%20844575a5671b427db8b7d07ae97e8502/Untitled%202.png)